{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mliu/anaconda3/lib/python3.11/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys \n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "import re\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch import nn\n",
    "from torch.utils import data as torch_data\n",
    "\n",
    "from sklearn import model_selection as sk_model_selection\n",
    "from torch.nn import functional as torch_functional\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import nibabel as nib\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor,ViTImageProcessor\n",
    "from trainervit import Trainer\n",
    "\n",
    "\n",
    "tcformerpath = \"TCFormer-master/classification\"\n",
    "sys.path.append(tcformerpath)\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import json\n",
    "import losses\n",
    "\n",
    "from pathlib import Path\n",
    "import timm\n",
    "\n",
    "\n",
    "from timm.data import Mixup\n",
    "from timm.models import create_model\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "from timm.scheduler import create_scheduler\n",
    "from timm.optim import create_optimizer\n",
    "from timm.utils import NativeScaler\n",
    "\n",
    "# from datasets import build_dataset\n",
    "from engine import train_one_epoch, evaluate\n",
    "from losses import DistillationLoss\n",
    "import utils\n",
    "import samplers\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import tcformer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, ResNetForImageClassification\n",
    "\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = 'input-2/output_slices/output_slices_DWI_BET'\n",
    "data_im_folder = data_directory \n",
    "SIZE = 224\n",
    "df = pd.read_csv('input-2/csv/labels_2classes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing key head.weight from pretrained checkpoint\n",
      "Removing key head.bias from pretrained checkpoint\n",
      "Checkpoint loaded successfully\n",
      "TCFormer(\n",
      "  (patch_embed1): OverlapPatchEmbed(\n",
      "    (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
      "    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (block1): ModuleList(\n",
      "    (0-1): 2 x Block(\n",
      "      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (kv): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=64, out_features=512, bias=True)\n",
      "        (dwconv): DWConv(\n",
      "          (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "  (ctm1): CTM(\n",
      "    (conv): TokenConv(\n",
      "      64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
      "      (skip): Conv1d(64, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    )\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (score): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      "  (block2): ModuleList(\n",
      "    (0-1): 2 x TCBlock(\n",
      "      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): TCAttention(\n",
      "        (q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (kv): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): TCMlp(\n",
      "        (fc1): Linear(in_features=128, out_features=1024, bias=True)\n",
      "        (dwconv): TokenConv(\n",
      "          1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024\n",
      "          (skip): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), groups=1024, bias=False)\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "  (ctm2): CTM(\n",
      "    (conv): TokenConv(\n",
      "      128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
      "      (skip): Conv1d(128, 320, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    )\n",
      "    (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "    (score): Linear(in_features=320, out_features=1, bias=True)\n",
      "  )\n",
      "  (block3): ModuleList(\n",
      "    (0-1): 2 x TCBlock(\n",
      "      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): TCAttention(\n",
      "        (q): Linear(in_features=320, out_features=320, bias=True)\n",
      "        (kv): Linear(in_features=320, out_features=640, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=320, out_features=320, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
      "        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): TCMlp(\n",
      "        (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "        (dwconv): TokenConv(\n",
      "          1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280\n",
      "          (skip): Conv1d(1280, 1280, kernel_size=(1,), stride=(1,), groups=1280, bias=False)\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm3): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "  (ctm3): CTM(\n",
      "    (conv): TokenConv(\n",
      "      320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
      "      (skip): Conv1d(320, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (score): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      "  (block4): ModuleList(\n",
      "    (0-1): 2 x TCBlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): TCAttention(\n",
      "        (q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): TCMlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dwconv): TokenConv(\n",
      "          2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048\n",
      "          (skip): Conv1d(2048, 2048, kernel_size=(1,), stride=(1,), groups=2048, bias=False)\n",
      "        )\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm4): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "  (head): Identity()\n",
      ")\n",
      "Total parameters: 13718659\n"
     ]
    }
   ],
   "source": [
    "model_name = 'tcformer_light'  \n",
    "input_size = 224  \n",
    "drop_rate = 0.0  \n",
    "drop_path_rate = 0.0  #\n",
    "num_classes = 1  \n",
    "finetune_path = 'tcformer_light-edacd9e5_20220606.pth'\n",
    "\n",
    "\n",
    "model = create_model(\n",
    "    model_name,\n",
    "    pretrained=False,\n",
    "    num_classes=num_classes,\n",
    "    drop_rate=drop_rate,\n",
    "    drop_path_rate=drop_path_rate,\n",
    "    drop_block_rate=None,\n",
    ")\n",
    "\n",
    "if finetune_path:\n",
    "    if finetune_path.startswith('https'):\n",
    "      \n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            finetune_path, map_location='cpu', check_hash=True)\n",
    "    else:\n",
    "       \n",
    "        checkpoint = torch.load(finetune_path, map_location='cpu')\n",
    "\n",
    "   \n",
    "    if 'model' in checkpoint:\n",
    "        checkpoint_model = checkpoint['model']\n",
    "    else:\n",
    "        checkpoint_model = checkpoint\n",
    "\n",
    "  \n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    # remove head\n",
    "    for k in ['head.weight', 'head.bias', 'head_dist.weight', 'head_dist.bias']:\n",
    "        if k in checkpoint_model and checkpoint_model[k].shape != state_dict[k].shape:\n",
    "            print(f\"Removing key {k} from pretrained checkpoint\")\n",
    "            del checkpoint_model[k]\n",
    "\n",
    "   \n",
    "    _ = model.load_state_dict(checkpoint_model, strict=False)\n",
    "    print(\"Checkpoint loaded successfully\")\n",
    "\n",
    "if hasattr(model, 'head'):\n",
    "    model.head = torch.nn.Identity() \n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "print(model)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_png_images(scan_id, image_type='Yellow_slice', image_size=(224, 224)):\n",
    "    image_filename = f\"{scan_id}/{image_type}.png\"\n",
    "    image_path = os.path.join(data_im_folder, image_filename)\n",
    "\n",
    "   \n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    _, thresh = cv2.threshold(image, 10, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Remove excessive black background\n",
    "    non_zero_columns = np.sum(thresh > 0, axis=0)\n",
    "    left_bound = np.argmax(non_zero_columns > 0)\n",
    "    right_bound = len(non_zero_columns) - np.argmax(non_zero_columns[::-1] > 0)\n",
    "    margin = 20 \n",
    "    left_bound = max(0, left_bound - margin)\n",
    "    right_bound = min(image.shape[1], right_bound + margin)\n",
    "\n",
    "   \n",
    "    cropped_image = image[:, left_bound:right_bound]\n",
    "\n",
    "    resized_image = cv2.resize(cropped_image, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    resized_image_pil = Image.fromarray(resized_image)\n",
    "\n",
    "  \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.Grayscale(num_output_channels=3),  \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n",
    "    ])\n",
    "\n",
    "  \n",
    "    image_tensor = transform(resized_image_pil)\n",
    "    # image_tensor = image_tensor.unsqueeze(0) \n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of patients：126\n",
      "\n",
      "processing 1 patient，ID：p0001\n",
      "read p0001 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 2 patient，ID：p0006\n",
      "read p0006 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 3 patient，ID：p0008\n",
      "read p0008 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 4 patient，ID：p0013\n",
      "read p0013 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 5 patient，ID：p0014\n",
      "read p0014 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 6 patient，ID：p0017\n",
      "read p0017 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 7 patient，ID：p0021\n",
      "read p0021 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 8 patient，ID：p0023\n",
      "read p0023 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 9 patient，ID：p0024\n",
      "read p0024 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 10 patient，ID：p0025\n",
      "read p0025 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 11 patient，ID：p0026\n",
      "read p0026 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 12 patient，ID：p0027\n",
      "read p0027 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 13 patient，ID：p0031\n",
      "read p0031 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 14 patient，ID：p0033\n",
      "read p0033 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 15 patient，ID：p0036\n",
      "read p0036 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 16 patient，ID：p0038\n",
      "read p0038 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 17 patient，ID：p0040\n",
      "read p0040 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 18 patient，ID：p0043\n",
      "read p0043 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 19 patient，ID：p0046\n",
      "read p0046 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 20 patient，ID：p0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_160299/3699842695.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  patient_data_tensor = torch.tensor(patient_data, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read p0048 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 21 patient，ID：p0051\n",
      "read p0051 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 22 patient，ID：p0056\n",
      "read p0056 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 23 patient，ID：p0058\n",
      "read p0058 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 24 patient，ID：p0062\n",
      "read p0062 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 25 patient，ID：p0063\n",
      "read p0063 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 26 patient，ID：p0065\n",
      "read p0065 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 27 patient，ID：p0068\n",
      "read p0068 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 28 patient，ID：p0071\n",
      "read p0071 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 29 patient，ID：p0077\n",
      "read p0077 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 30 patient，ID：p0081\n",
      "read p0081 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 31 patient，ID：p0092\n",
      "read p0092 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 32 patient，ID：p0099\n",
      "read p0099 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 33 patient，ID：p0101\n",
      "read p0101 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 34 patient，ID：p0104\n",
      "read p0104 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 35 patient，ID：p0106\n",
      "read p0106 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 36 patient，ID：p0113\n",
      "read p0113 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 37 patient，ID：p0114\n",
      "read p0114 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 38 patient，ID：p0118\n",
      "read p0118 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 39 patient，ID：p0119\n",
      "read p0119 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 40 patient，ID：p0125\n",
      "read p0125 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 41 patient，ID：p0132\n",
      "read p0132 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 42 patient，ID：p0139\n",
      "read p0139 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 43 patient，ID：p0144\n",
      "read p0144 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 44 patient，ID：p0145\n",
      "read p0145 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 45 patient，ID：p0149\n",
      "read p0149 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 46 patient，ID：p0150\n",
      "read p0150 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 47 patient，ID：p0153\n",
      "read p0153 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 48 patient，ID：p0157\n",
      "read p0157 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 49 patient，ID：p0159\n",
      "read p0159 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 50 patient，ID：p0160\n",
      "read p0160 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 51 patient，ID：p0162\n",
      "read p0162 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 52 patient，ID：p0163\n",
      "read p0163 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 53 patient，ID：p0164\n",
      "read p0164 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 54 patient，ID：p0165\n",
      "read p0165 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 55 patient，ID：p0166\n",
      "read p0166 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 56 patient，ID：p0167\n",
      "read p0167 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 57 patient，ID：p0175\n",
      "read p0175 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 58 patient，ID：p0180\n",
      "read p0180 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 59 patient，ID：p0182\n",
      "read p0182 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 60 patient，ID：p0183\n",
      "read p0183 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 61 patient，ID：p0185\n",
      "read p0185 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 62 patient，ID：p0186\n",
      "read p0186 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 63 patient，ID：p0187\n",
      "read p0187 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 64 patient，ID：p0188\n",
      "read p0188 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 65 patient，ID：p0189\n",
      "read p0189 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 66 patient，ID：p0192\n",
      "read p0192 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 67 patient，ID：p0193\n",
      "read p0193 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 68 patient，ID：p0195\n",
      "read p0195 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 69 patient，ID：p0197\n",
      "read p0197 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 70 patient，ID：p0198\n",
      "read p0198 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 71 patient，ID：p0200\n",
      "read p0200 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 72 patient，ID：p0202\n",
      "read p0202 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 73 patient，ID：p0203\n",
      "read p0203 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 74 patient，ID：p0207\n",
      "read p0207 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 75 patient，ID：p0208\n",
      "read p0208 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 76 patient，ID：p0212\n",
      "read p0212 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 77 patient，ID：p0213\n",
      "read p0213 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 78 patient，ID：p0216\n",
      "read p0216 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 79 patient，ID：p0217\n",
      "read p0217 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 80 patient，ID：p0219\n",
      "read p0219 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 81 patient，ID：p0221\n",
      "read p0221 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 82 patient，ID：p0223\n",
      "read p0223 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 83 patient，ID：p0224\n",
      "read p0224 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 84 patient，ID：p0226\n",
      "read p0226 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 85 patient，ID：p0228\n",
      "read p0228 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 86 patient，ID：p0231\n",
      "read p0231 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 87 patient，ID：p0232\n",
      "read p0232 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 88 patient，ID：p0235\n",
      "read p0235 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 89 patient，ID：p0236\n",
      "read p0236 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 90 patient，ID：p0238\n",
      "read p0238 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 91 patient，ID：p0239\n",
      "read p0239 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 92 patient，ID：p0241\n",
      "read p0241 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 93 patient，ID：p0242\n",
      "read p0242 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 94 patient，ID：p0250\n",
      "read p0250 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 95 patient，ID：p0251\n",
      "read p0251 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 96 patient，ID：p0253\n",
      "read p0253 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 97 patient，ID：p0254\n",
      "read p0254 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 98 patient，ID：p0257\n",
      "read p0257 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 99 patient，ID：p0262\n",
      "read p0262 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 100 patient，ID：p0263\n",
      "read p0263 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 101 patient，ID：p0266\n",
      "read p0266 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 102 patient，ID：p0268\n",
      "read p0268 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 103 patient，ID：p0269\n",
      "read p0269 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 104 patient，ID：p0272\n",
      "read p0272 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 105 patient，ID：p0274\n",
      "read p0274 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 106 patient，ID：p0276\n",
      "read p0276 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 107 patient，ID：p0280\n",
      "read p0280 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 108 patient，ID：p0283\n",
      "read p0283 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 109 patient，ID：p0284\n",
      "read p0284 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 110 patient，ID：p0285\n",
      "read p0285 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 111 patient，ID：p0286\n",
      "read p0286 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 112 patient，ID：p0287\n",
      "read p0287 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 113 patient，ID：p0288\n",
      "read p0288 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 114 patient，ID：p0290\n",
      "read p0290 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 115 patient，ID：p0291\n",
      "read p0291 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 116 patient，ID：p0294\n",
      "read p0294 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 117 patient，ID：p0297\n",
      "read p0297 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 118 patient，ID：p0298\n",
      "read p0298 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 119 patient，ID：p0299\n",
      "read p0299 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 120 patient，ID：p0301\n",
      "read p0301 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 121 patient，ID：p0302\n",
      "read p0302 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 122 patient，ID：p0303\n",
      "read p0303 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 123 patient，ID：p0305\n",
      "read p0305 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 124 patient，ID：p0306\n",
      "read p0306 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 125 patient，ID：p0307\n",
      "read p0307 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "processing 126 patient，ID：p0308\n",
      "read p0308 data， tensor shape：torch.Size([3, 224, 224])\n",
      "\n",
      "all patients are read sucussfully.\n",
      "final tensor shape：torch.Size([126, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "patient_ids = df[\"patient_id\"].values\n",
    "all_patient_data = []\n",
    "\n",
    "\n",
    "print(f\"total number of patients：{len(patient_ids)}\\n\")\n",
    "\n",
    "for idx, patient_id in enumerate(patient_ids):\n",
    "    print(f\"processing {idx + 1} patient，ID：{patient_id}\")\n",
    "    \n",
    "   \n",
    "    patient_data = load_png_images(patient_id) \n",
    "   \n",
    "  \n",
    "    patient_data_tensor = torch.tensor(patient_data, dtype=torch.float32)\n",
    "    \n",
    "   \n",
    "    all_patient_data.append(patient_data_tensor)\n",
    "\n",
    "    \n",
    "    print(f\"read {patient_id} data， tensor shape：{patient_data_tensor.shape}\\n\")\n",
    "\n",
    "\n",
    "all_patient_data_tensor = torch.stack(all_patient_data)\n",
    "\n",
    "print(\"all patients are read sucussfully.\")\n",
    "print(f\"final tensor shape：{all_patient_data_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mliu/VIT/TCFormer-master/classification/tcformer_module/tcformer_utils.py:179: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)\n",
      "  A = torch.sparse.FloatTensor(coor, value, torch.Size([B * H * W, B * N]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: torch.Size([126, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def extract_patient_features(data_tensor, model, batch_size=16):\n",
    "    \"\"\"\n",
    "    Extract features from a data tensor using the provided model.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Extracted features of shape (N, feature_dim).\n",
    "    \"\"\"\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    device = torch.device('cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    data_tensor = data_tensor.to(device)\n",
    "    dataset = TensorDataset(data_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    all_features = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Get the batch data and move it to the device\n",
    "            inputs = batch[0].to(device)\n",
    "            \n",
    "            # Forward pass through the model\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "           \n",
    "          
    "            # features = outputs.logits # Shape: (batch_size, feature_dim)\n",
    "            features = outputs \n",
    "            all_features.append(features)\n",
    "    \n",
    "    # Concatenate all features to form a tensor of shape (N, feature_dim)\n",
    "    all_features_tensor = torch.cat(all_features, dim=0)\n",
    "    \n",
    "    return all_features_tensor\n",
    "\n",
    "batch_size = 16# Adjust as needed\n",
    "\n",
    "# Call the feature extraction function\n",
    "features_tensor = extract_patient_features(all_patient_data_tensor, model, batch_size)\n",
    "\n",
    "\n",
    "# Check the shape of the extracted features\n",
    "print(f\"Extracted features shape: {features_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features save sucussfully\n"
     ]
    }
   ],
   "source": [
    "features_array = features_tensor.numpy()\n",
    "df_features = pd.DataFrame(features_array)\n",
    "\n",
    "# add patients ID\n",
    "df_features.insert(0, 'Patient_ID', patient_ids)\n",
    "\n",
    "df_features.to_csv('features_yellow_example.csv', index=False)\n",
    "\n",
    "print(\"features save sucussfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
