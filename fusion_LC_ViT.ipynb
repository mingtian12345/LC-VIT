{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys \n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils import data as torch_data\n",
    "\n",
    "from sklearn import model_selection as sk_model_selection\n",
    "from torch.nn import functional as torch_functional\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,roc_curve,mean_absolute_error,auc,confusion_matrix\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import pandas as pd\n",
    "\n",
    "print('ok')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False  \n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    patient_id  label\n",
      "0        p0001      1\n",
      "1        p0006      0\n",
      "2        p0008      0\n",
      "3        p0013      0\n",
      "4        p0014      0\n",
      "..         ...    ...\n",
      "114      p0291      0\n",
      "115      p0294      1\n",
      "116      p0302      0\n",
      "117      p0306      0\n",
      "118      p0308      0\n",
      "\n",
      "[119 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'p0036': 0,\n",
       " 'p0058': 0,\n",
       " 'p0081': 0,\n",
       " 'p0149': 0,\n",
       " 'p0162': 0,\n",
       " 'p0164': 0,\n",
       " 'p0185': 0,\n",
       " 'p0188': 0,\n",
       " 'p0198': 0,\n",
       " 'p0250': 0,\n",
       " 'p0283': 0,\n",
       " 'p0291': 0,\n",
       " 'p0027': 1,\n",
       " 'p0065': 1,\n",
       " 'p0101': 1,\n",
       " 'p0114': 1,\n",
       " 'p0144': 1,\n",
       " 'p0159': 1,\n",
       " 'p0165': 1,\n",
       " 'p0219': 1,\n",
       " 'p0262': 1,\n",
       " 'p0274': 1,\n",
       " 'p0286': 1,\n",
       " 'p0287': 1,\n",
       " 'p0013': 2,\n",
       " 'p0021': 2,\n",
       " 'p0062': 2,\n",
       " 'p0071': 2,\n",
       " 'p0113': 2,\n",
       " 'p0160': 2,\n",
       " 'p0193': 2,\n",
       " 'p0197': 2,\n",
       " 'p0208': 2,\n",
       " 'p0224': 2,\n",
       " 'p0266': 2,\n",
       " 'p0302': 2,\n",
       " 'p0033': 3,\n",
       " 'p0043': 3,\n",
       " 'p0092': 3,\n",
       " 'p0104': 3,\n",
       " 'p0119': 3,\n",
       " 'p0145': 3,\n",
       " 'p0180': 3,\n",
       " 'p0183': 3,\n",
       " 'p0200': 3,\n",
       " 'p0228': 3,\n",
       " 'p0236': 3,\n",
       " 'p0285': 3,\n",
       " 'p0014': 4,\n",
       " 'p0025': 4,\n",
       " 'p0046': 4,\n",
       " 'p0063': 4,\n",
       " 'p0077': 4,\n",
       " 'p0187': 4,\n",
       " 'p0212': 4,\n",
       " 'p0223': 4,\n",
       " 'p0235': 4,\n",
       " 'p0239': 4,\n",
       " 'p0263': 4,\n",
       " 'p0269': 4,\n",
       " 'p0001': 5,\n",
       " 'p0008': 5,\n",
       " 'p0150': 5,\n",
       " 'p0157': 5,\n",
       " 'p0167': 5,\n",
       " 'p0202': 5,\n",
       " 'p0213': 5,\n",
       " 'p0231': 5,\n",
       " 'p0241': 5,\n",
       " 'p0254': 5,\n",
       " 'p0276': 5,\n",
       " 'p0306': 5,\n",
       " 'p0031': 6,\n",
       " 'p0038': 6,\n",
       " 'p0048': 6,\n",
       " 'p0068': 6,\n",
       " 'p0163': 6,\n",
       " 'p0175': 6,\n",
       " 'p0192': 6,\n",
       " 'p0242': 6,\n",
       " 'p0257': 6,\n",
       " 'p0272': 6,\n",
       " 'p0284': 6,\n",
       " 'p0288': 6,\n",
       " 'p0006': 7,\n",
       " 'p0023': 7,\n",
       " 'p0051': 7,\n",
       " 'p0099': 7,\n",
       " 'p0106': 7,\n",
       " 'p0118': 7,\n",
       " 'p0125': 7,\n",
       " 'p0186': 7,\n",
       " 'p0189': 7,\n",
       " 'p0217': 7,\n",
       " 'p0238': 7,\n",
       " 'p0290': 7,\n",
       " 'p0024': 8,\n",
       " 'p0026': 8,\n",
       " 'p0056': 8,\n",
       " 'p0132': 8,\n",
       " 'p0139': 8,\n",
       " 'p0182': 8,\n",
       " 'p0203': 8,\n",
       " 'p0207': 8,\n",
       " 'p0216': 8,\n",
       " 'p0221': 8,\n",
       " 'p0226': 8,\n",
       " 'p0251': 8,\n",
       " 'p0017': 9,\n",
       " 'p0040': 9,\n",
       " 'p0153': 9,\n",
       " 'p0166': 9,\n",
       " 'p0195': 9,\n",
       " 'p0232': 9,\n",
       " 'p0253': 9,\n",
       " 'p0268': 9,\n",
       " 'p0280': 9,\n",
       " 'p0294': 9,\n",
       " 'p0308': 9}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = pd.read_csv('input-2/csv/labels_2classes.csv')\n",
    "df_clinical_encoded = pd.read_excel('df_clinical_encoded.xlsx')\n",
    "df_filtered = df_1[~df_1['patient_id'].isin(['p0297', 'p0298', 'p0299', 'p0303', 'p0307','p0305','p0301'])]          ##### No clinical data for these patients\n",
    "df_filtered = df_filtered.reset_index(drop=True)\n",
    "print(df_filtered)\n",
    "patient_ids = df_filtered['patient_id'].values  # get patient_id \n",
    "\n",
    "# put patient_id  to the first column of  df_clinical_encoded \n",
    "df_clinical_encoded.insert(0, 'patient_id', patient_ids)\n",
    "\n",
    "\n",
    "#### Create 10 fold list for all patients\n",
    "num_folds = 10\n",
    "patient_labels = df_filtered.groupby('patient_id')['label'].first().reset_index()\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "patient_ids = patient_labels['patient_id'].values\n",
    "patient_y = patient_labels['label'].values\n",
    "\n",
    "#split patient ID\n",
    "fold_indices = list(skf.split(patient_ids, patient_y))\n",
    "\n",
    "\n",
    "patient_fold_mapping = {}\n",
    "for fold, (train_idx, test_idx) in enumerate(fold_indices):\n",
    "    test_patient_ids = patient_ids[test_idx]\n",
    "    for pid in test_patient_ids:\n",
    "        patient_fold_mapping[pid] = fold\n",
    "\n",
    "\n",
    "patient_fold_mapping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exclude_ids = ['p0297', 'p0298', 'p0299', 'p0303', 'p0307', 'p0305', 'p0301']\n",
    "\n",
    "# Define a function to read, filter, and rename columns  , Patient_ID  //// patient_id \n",
    "def load_and_process(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    if 'Patient_ID' in df.columns:\n",
    "        df = df.rename(columns={'Patient_ID': 'patient_id'})\n",
    "    \n",
    "    elif 'patient_id' not in df.columns:\n",
    "        raise ValueError(f\"{file_path} NO 'Patient_ID'  or  'patient_id' \")\n",
    "    \n",
    "\n",
    "    df_filtered = df[~df['patient_id'].isin(exclude_ids)]\n",
    "    df_filtered = df_filtered.reset_index(drop=True)\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# process the 3 image features\n",
    "df_red = load_and_process('features_red.csv')\n",
    "df_green = load_and_process('features_green.csv')\n",
    "df_yellow = load_and_process('features_yellow.csv')\n",
    "\n",
    "\n",
    "\n",
    "# delete 'red_features'、'green_features'  'yellow_features' （if already exist）\n",
    "if 'red_features' in df_red.columns:\n",
    "    df_red = df_red.drop(['red_features'], axis=1)\n",
    "if 'green_features' in df_green.columns:\n",
    "    df_green = df_green.drop(['green_features'], axis=1)\n",
    "if 'yellow_features' in df_yellow.columns:\n",
    "    df_yellow = df_yellow.drop(['yellow_features'], axis=1)\n",
    "\n",
    "# create feature column\n",
    "df_red['red_features'] = df_red.iloc[:, 1:].values.tolist()\n",
    "df_green['green_features'] = df_green.iloc[:, 1:].values.tolist()\n",
    "df_yellow['yellow_features'] = df_yellow.iloc[:, 1:].values.tolist()\n",
    "\n",
    "# merge image features in one dataframe\n",
    "df_images = pd.merge(df_red[['patient_id', 'red_features']], \n",
    "                     df_green[['patient_id', 'green_features']], \n",
    "                     on='patient_id', how='inner')\n",
    "df_images = pd.merge(df_images, df_yellow[['patient_id', 'yellow_features']], \n",
    "                     on='patient_id', how='inner')\n",
    "\n",
    "# merge all\n",
    "df = pd.merge(df_clinical_encoded, df_images, on='patient_id', how='inner')\n",
    "df = pd.merge(df, df_filtered[['patient_id', 'label']], on='patient_id', how='inner')\n",
    "\n",
    "# Determine feature columns and label columns, and exclude unnecessary columns\n",
    "clinical_feature_cols = [col for col in df.columns if col not in ['patient_id', 'label', 'red_features', 'green_features', 'yellow_features']]\n",
    "label_col = 'label'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClinicalImageDataset(Dataset):\n",
    "    def __init__(self, df, clinical_feature_cols, label_col):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): \n",
    "            clinical_clinical_feature_cols (list): \n",
    "            label_col (str): \n",
    "        \"\"\"\n",
    "        # clinical feature\n",
    "        self.X_clinical = torch.tensor(df[clinical_feature_cols].values, dtype=torch.float32)\n",
    "        \n",
    "        # image features\n",
    "        self.X_image_red = torch.from_numpy(np.stack(df['red_features'].values)).float()  # (B, 512)\n",
    "        self.X_image_green = torch.from_numpy(np.stack(df['green_features'].values)).float()  # (B, 512)\n",
    "        self.X_image_yellow = torch.from_numpy(np.stack(df['yellow_features'].values)).float()  # (B, 512)\n",
    "        \n",
    "        # label\n",
    "        self.y = torch.tensor(df[label_col].values, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X_clinical[idx],\n",
    "                self.X_image_red[idx],\n",
    "                self.X_image_green[idx],\n",
    "                self.X_image_yellow[idx],\n",
    "                self.y[idx])\n",
    "\n",
    "# # 3. define  simple cross attention\n",
    "# class CrossAttentionModule(nn.Module):\n",
    "#     def __init__(self, embed_dim=512, num_heads=8, dropout=0.1):\n",
    "#         super(CrossAttentionModule, self).__init__()\n",
    "#         self.cross_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
    "#         self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "#         self.feed_forward = nn.Sequential(\n",
    "#             nn.Linear(embed_dim, embed_dim * 2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(embed_dim * 2, embed_dim)\n",
    "#         )\n",
    "#         self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "#     def forward(self, queries, keys, values):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             queries: (B, Q_len, E) - \n",
    "#             keys: (B, K_len, E) - \n",
    "#             values: (B, K_len, E) - \n",
    "#         Returns:\n",
    "#             out2: (B, Q_len, E) - \n",
    "#             attn_weights: (B, Q_len, K_len) \n",
    "#         \"\"\"\n",
    "#         attn_output, attn_weights = self.cross_attn(queries, keys, values)  # (B, Q_len, E), (B, Q_len, K_len)\n",
    "#         attn_output = self.dropout(attn_output)\n",
    "#         out1 = self.layer_norm1(queries + attn_output) \n",
    "        \n",
    "#         ff_output = self.feed_forward(out1)  # (B, Q_len, E)\n",
    "#         ff_output = self.dropout(ff_output)\n",
    "#         out2 = self.layer_norm2(out1 + ff_output)  \n",
    "        \n",
    "#         return out2, attn_weights\n",
    "\n",
    "### mutual cross attention\n",
    "class MutualCrossAttentionModule(nn.Module):\n",
    "    def __init__(self, embed_dim=512, num_heads=8, dropout=0.1):\n",
    "        super(MutualCrossAttentionModule, self).__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim * 2, embed_dim)\n",
    "        )\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"\n",
    "        x1: (B, len1, E)\n",
    "        x2: (B, len2, E)\n",
    "        \n",
    "        Mutual:\n",
    "        1. x1=Q, x2 as K,V, get output_A\n",
    "        2. x2=Q, x1 as K,V,g et output_B\n",
    "        output = output_A + output_B\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        # x1->x2\n",
    "        output_A, attn_weights_A = self.mha(x1, x2, x2)  # (B, len1, E), (B, len1, len2)\n",
    "        # x2->x1\n",
    "        output_B, attn_weights_B = self.mha(x2, x1, x1)  # (B, len2, E), (B, len2, len1)\n",
    "\n",
    "        \n",
    "        output = output_A + output_B  # (B, len1, E)\n",
    "\n",
    "      \n",
    "        out1 = self.layer_norm1(x1 + output)\n",
    "        ff_output = self.feed_forward(out1)\n",
    "        ff_output = self.dropout(ff_output)\n",
    "        out2 = self.layer_norm2(out1 + ff_output)\n",
    "\n",
    "       \n",
    "        return out2, attn_weights_A\n",
    "\n",
    "\n",
    "# 4. define fusion model\n",
    "class ClinicalImageFusionModel(nn.Module):\n",
    "    def __init__(self, clinical_dim=85, embed_dim=512, num_heads=4, dropout=0.1):\n",
    "        super(ClinicalImageFusionModel, self).__init__()\n",
    "        # clinical MLP: 85 -> 512\n",
    "        self.clinical_mlp = nn.Sequential(\n",
    "            nn.Linear(clinical_dim, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # cross attention\n",
    "        self.cross_attn = MutualCrossAttentionModule(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout)\n",
    "        \n",
    "        # classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 1)  # logit output\n",
    "        )\n",
    "    \n",
    "    def forward(self, clinical_features, image_red, image_green, image_yellow):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            clinical_features: (B, 85)\n",
    "            image_red: (B, 512)\n",
    "            image_green: (B, 512)\n",
    "            image_yellow: (B, 512)\n",
    "        Returns:\n",
    "            logits: (B, 1)\n",
    "            attn_weights: (B, 3, 3)\n",
    "        \"\"\"\n",
    "        # encode clinical\n",
    "        clinical_embed = self.clinical_mlp(clinical_features)  # (B, 512)\n",
    "        \n",
    "        # copy 3  clinical times\n",
    "        clinical_embed = clinical_embed.unsqueeze(1).repeat(1, 3, 1)  # (B, 3, 512)\n",
    "        \n",
    "        # merge image features\n",
    "        image_features = torch.stack([image_red, image_green, image_yellow], dim=1)  # (B, 3, 512)\n",
    "        \n",
    "        # cross attetion \n",
    "        fused_features, attn_weights = self.cross_attn(clinical_embed, image_features)  # (B, 3, 512), (B, 3, 3)\n",
    "        \n",
    "        # merge fusion features\n",
    "        fused_features = fused_features.mean(dim=1)  # (B, 512)\n",
    "        \n",
    "        # classifier \n",
    "        logits = self.classifier(fused_features)  # (B, 1)\n",
    "        \n",
    "        return logits, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for X_clinical, X_red, X_green, X_yellow, y_batch in dataloader:\n",
    "        # to GPU\n",
    "        X_clinical = X_clinical.to(device)\n",
    "        X_red = X_red.to(device)\n",
    "        X_green = X_green.to(device)\n",
    "        X_yellow = X_yellow.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        y_batch = y_batch.float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(X_clinical, X_red, X_green, X_yellow)  # [B, 1]\n",
    "        outputs = outputs.squeeze(1)  # [B]\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * X_clinical.size(0)\n",
    "        \n",
    "        #  train ACC\n",
    "        predicted = (torch.sigmoid(outputs) > 0.36).long()\n",
    "        correct += (predicted == y_batch.long()).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_auc(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for X_clinical, X_red, X_green, X_yellow, y_batch in dataloader:\n",
    "            #  to GPU\n",
    "            X_clinical = X_clinical.to(device)\n",
    "            X_red = X_red.to(device)\n",
    "            X_green = X_green.to(device)\n",
    "            X_yellow = X_yellow.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            outputs, _ = model(X_clinical, X_red, X_green, X_yellow)  # [B, 1]\n",
    "            outputs = outputs.squeeze(1)\n",
    "            probas = torch.sigmoid(outputs).cpu().numpy()  \n",
    "            all_outputs.append(probas)\n",
    "            all_targets.append(y_batch.cpu().numpy())\n",
    "\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    auc = roc_auc_score(all_targets, all_outputs)\n",
    "    return auc\n",
    "\n",
    "def test_inference(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for X_clinical, X_red, X_green, X_yellow, y_batch in dataloader:\n",
    "          \n",
    "            X_clinical = X_clinical.to(device)\n",
    "            X_red = X_red.to(device)\n",
    "            X_green = X_green.to(device)\n",
    "            X_yellow = X_yellow.to(device)\n",
    "\n",
    "            outputs, _ = model(X_clinical, X_red, X_green, X_yellow)  # [B, 1]\n",
    "            outputs = outputs.squeeze(1)  # [B]\n",
    "            probas = torch.sigmoid(outputs).cpu().numpy()  \n",
    "            all_preds.append(probas)\n",
    "            all_targets.append(y_batch.numpy())\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    return all_targets, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1/10...\n",
      "Fold 1 IDs: {'train_ids': ['p0027', 'p0065', 'p0101', 'p0114', 'p0144', 'p0159', 'p0165', 'p0219', 'p0262', 'p0274', 'p0286', 'p0287', 'p0013', 'p0021', 'p0062', 'p0071', 'p0113', 'p0160', 'p0193', 'p0197', 'p0208', 'p0224', 'p0266', 'p0302', 'p0033', 'p0043', 'p0092', 'p0104', 'p0119', 'p0145', 'p0180', 'p0183', 'p0200', 'p0228', 'p0236', 'p0285', 'p0014', 'p0025', 'p0046', 'p0063', 'p0077', 'p0187', 'p0212', 'p0223', 'p0235', 'p0239', 'p0263', 'p0269', 'p0001', 'p0008', 'p0150', 'p0157', 'p0167', 'p0202', 'p0213', 'p0231', 'p0241', 'p0254', 'p0276', 'p0306', 'p0031', 'p0038', 'p0048', 'p0068', 'p0163', 'p0175', 'p0192', 'p0242', 'p0257', 'p0272', 'p0284', 'p0288', 'p0006', 'p0023', 'p0051', 'p0099', 'p0106', 'p0118', 'p0125', 'p0186', 'p0189', 'p0217', 'p0238', 'p0290', 'p0024', 'p0026', 'p0056', 'p0132', 'p0139', 'p0182', 'p0203', 'p0207', 'p0216', 'p0221', 'p0226', 'p0251'], 'val_ids': ['p0017', 'p0040', 'p0153', 'p0166', 'p0195', 'p0232', 'p0253', 'p0268', 'p0280', 'p0294', 'p0308'], 'test_ids': ['p0036', 'p0058', 'p0081', 'p0149', 'p0162', 'p0164', 'p0185', 'p0188', 'p0198', 'p0250', 'p0283', 'p0291']}\n",
      "Fold[1] Epoch [1/100] Train Loss: 0.6102 Train Acc: 0.6146, Val AUC: 0.4643\n",
      "Fold[1] Epoch [2/100] Train Loss: 0.5687 Train Acc: 0.7292, Val AUC: 0.5357\n",
      "Fold[1] Epoch [3/100] Train Loss: 0.4967 Train Acc: 0.7708, Val AUC: 0.5000\n",
      "Fold[1] Epoch [4/100] Train Loss: 0.4482 Train Acc: 0.8854, Val AUC: 0.5000\n",
      "Fold[1] Epoch [5/100] Train Loss: 0.3916 Train Acc: 0.8438, Val AUC: 0.5357\n",
      "Fold[1] Epoch [6/100] Train Loss: 0.3312 Train Acc: 0.8750, Val AUC: 0.6071\n",
      "Fold[1] Epoch [7/100] Train Loss: 0.2484 Train Acc: 0.9375, Val AUC: 0.6429\n",
      "Fold[1] Epoch [8/100] Train Loss: 0.1673 Train Acc: 0.9688, Val AUC: 0.7143\n",
      "Fold[1] Epoch [9/100] Train Loss: 0.1246 Train Acc: 0.9896, Val AUC: 0.7143\n",
      "Fold[1] Epoch [10/100] Train Loss: 0.0679 Train Acc: 1.0000, Val AUC: 0.7143\n",
      "Fold[1] Epoch [11/100] Train Loss: 0.0529 Train Acc: 1.0000, Val AUC: 0.7143\n",
      "Fold[1] Epoch [12/100] Train Loss: 0.0400 Train Acc: 1.0000, Val AUC: 0.7500\n",
      "Fold[1] Epoch [13/100] Train Loss: 0.0230 Train Acc: 1.0000, Val AUC: 0.7857\n",
      "Fold[1] Epoch [14/100] Train Loss: 0.0265 Train Acc: 0.9896, Val AUC: 0.7857\n",
      "Fold[1] Epoch [15/100] Train Loss: 0.0285 Train Acc: 0.9896, Val AUC: 0.7857\n",
      "Fold[1] Epoch [16/100] Train Loss: 0.0129 Train Acc: 1.0000, Val AUC: 0.7500\n",
      "Fold[1] Epoch [17/100] Train Loss: 0.0086 Train Acc: 1.0000, Val AUC: 0.7500\n",
      "Fold[1] Epoch [18/100] Train Loss: 0.0098 Train Acc: 1.0000, Val AUC: 0.7500\n",
      "Fold[1] Epoch [19/100] Train Loss: 0.0136 Train Acc: 1.0000, Val AUC: 0.7500\n",
      "Fold[1] Epoch [20/100] Train Loss: 0.0043 Train Acc: 1.0000, Val AUC: 0.7500\n",
      "Fold[1] Epoch [21/100] Train Loss: 0.0049 Train Acc: 1.0000, Val AUC: 0.7857\n",
      "Fold[1] Epoch [22/100] Train Loss: 0.0045 Train Acc: 1.0000, Val AUC: 0.7857\n",
      "Fold[1] Epoch [23/100] Train Loss: 0.0099 Train Acc: 1.0000, Val AUC: 0.7857\n",
      "Validation AUC hasn't improved for 5 epochs. Early stopping...\n",
      "Fold[1] completed.\n",
      "\n",
      "Processing fold 2/10...\n",
      "Fold 2 IDs: {'train_ids': ['p0013', 'p0021', 'p0062', 'p0071', 'p0113', 'p0160', 'p0193', 'p0197', 'p0208', 'p0224', 'p0266', 'p0302', 'p0033', 'p0043', 'p0092', 'p0104', 'p0119', 'p0145', 'p0180', 'p0183', 'p0200', 'p0228', 'p0236', 'p0285', 'p0014', 'p0025', 'p0046', 'p0063', 'p0077', 'p0187', 'p0212', 'p0223', 'p0235', 'p0239', 'p0263', 'p0269', 'p0001', 'p0008', 'p0150', 'p0157', 'p0167', 'p0202', 'p0213', 'p0231', 'p0241', 'p0254', 'p0276', 'p0306', 'p0031', 'p0038', 'p0048', 'p0068', 'p0163', 'p0175', 'p0192', 'p0242', 'p0257', 'p0272', 'p0284', 'p0288', 'p0006', 'p0023', 'p0051', 'p0099', 'p0106', 'p0118', 'p0125', 'p0186', 'p0189', 'p0217', 'p0238', 'p0290', 'p0024', 'p0026', 'p0056', 'p0132', 'p0139', 'p0182', 'p0203', 'p0207', 'p0216', 'p0221', 'p0226', 'p0251', 'p0017', 'p0040', 'p0153', 'p0166', 'p0195', 'p0232', 'p0253', 'p0268', 'p0280', 'p0294', 'p0308'], 'val_ids': ['p0036', 'p0058', 'p0081', 'p0149', 'p0162', 'p0164', 'p0185', 'p0188', 'p0198', 'p0250', 'p0283', 'p0291'], 'test_ids': ['p0027', 'p0065', 'p0101', 'p0114', 'p0144', 'p0159', 'p0165', 'p0219', 'p0262', 'p0274', 'p0286', 'p0287']}\n",
      "Fold[2] Epoch [1/100] Train Loss: 0.6561 Train Acc: 0.5263, Val AUC: 0.7143\n",
      "Fold[2] Epoch [2/100] Train Loss: 0.5716 Train Acc: 0.7579, Val AUC: 0.7714\n",
      "Fold[2] Epoch [3/100] Train Loss: 0.5446 Train Acc: 0.7474, Val AUC: 0.7714\n",
      "Fold[2] Epoch [4/100] Train Loss: 0.4694 Train Acc: 0.8842, Val AUC: 0.7714\n",
      "Fold[2] Epoch [5/100] Train Loss: 0.4264 Train Acc: 0.9158, Val AUC: 0.7714\n",
      "Fold[2] Epoch [6/100] Train Loss: 0.3407 Train Acc: 0.9158, Val AUC: 0.7714\n",
      "Fold[2] Epoch [7/100] Train Loss: 0.2970 Train Acc: 0.9158, Val AUC: 0.8286\n",
      "Fold[2] Epoch [8/100] Train Loss: 0.2225 Train Acc: 0.9684, Val AUC: 0.8571\n",
      "Fold[2] Epoch [9/100] Train Loss: 0.1657 Train Acc: 0.9579, Val AUC: 0.9143\n",
      "Fold[2] Epoch [10/100] Train Loss: 0.1123 Train Acc: 0.9789, Val AUC: 0.9429\n",
      "Fold[2] Epoch [11/100] Train Loss: 0.0704 Train Acc: 1.0000, Val AUC: 0.9429\n",
      "Fold[2] Epoch [12/100] Train Loss: 0.0597 Train Acc: 1.0000, Val AUC: 0.9429\n",
      "Fold[2] Epoch [13/100] Train Loss: 0.0411 Train Acc: 0.9895, Val AUC: 0.9429\n",
      "Fold[2] Epoch [14/100] Train Loss: 0.0301 Train Acc: 0.9895, Val AUC: 0.9429\n",
      "Fold[2] Epoch [15/100] Train Loss: 0.0188 Train Acc: 1.0000, Val AUC: 0.9429\n",
      "Fold[2] Epoch [16/100] Train Loss: 0.0303 Train Acc: 0.9895, Val AUC: 0.9714\n",
      "Fold[2] Epoch [17/100] Train Loss: 0.0295 Train Acc: 0.9895, Val AUC: 0.9429\n",
      "Fold[2] Epoch [18/100] Train Loss: 0.0148 Train Acc: 1.0000, Val AUC: 0.9429\n",
      "Fold[2] Epoch [19/100] Train Loss: 0.0214 Train Acc: 0.9895, Val AUC: 1.0000\n",
      "Fold[2] Epoch [20/100] Train Loss: 0.0445 Train Acc: 0.9789, Val AUC: 1.0000\n",
      "Fold[2] Epoch [21/100] Train Loss: 0.0223 Train Acc: 1.0000, Val AUC: 0.9714\n",
      "Fold[2] Epoch [22/100] Train Loss: 0.0089 Train Acc: 1.0000, Val AUC: 0.9714\n",
      "Fold[2] Epoch [23/100] Train Loss: 0.0095 Train Acc: 1.0000, Val AUC: 0.9714\n",
      "Fold[2] Epoch [24/100] Train Loss: 0.0106 Train Acc: 0.9895, Val AUC: 0.9714\n",
      "Fold[2] Epoch [25/100] Train Loss: 0.0073 Train Acc: 1.0000, Val AUC: 0.9714\n",
      "Fold[2] Epoch [26/100] Train Loss: 0.0045 Train Acc: 1.0000, Val AUC: 0.9714\n",
      "Fold[2] Epoch [27/100] Train Loss: 0.0033 Train Acc: 1.0000, Val AUC: 0.9714\n",
      "Fold[2] Epoch [28/100] Train Loss: 0.0029 Train Acc: 1.0000, Val AUC: 0.9714\n",
      "Fold[2] Epoch [29/100] Train Loss: 0.0041 Train Acc: 1.0000, Val AUC: 0.9714\n",
      "Validation AUC hasn't improved for 5 epochs. Early stopping...\n",
      "Fold[2] completed.\n",
      "\n",
      "Processing fold 3/10...\n",
      "Fold 3 IDs: {'train_ids': ['p0036', 'p0058', 'p0081', 'p0149', 'p0162', 'p0164', 'p0185', 'p0188', 'p0198', 'p0250', 'p0283', 'p0291', 'p0033', 'p0043', 'p0092', 'p0104', 'p0119', 'p0145', 'p0180', 'p0183', 'p0200', 'p0228', 'p0236', 'p0285', 'p0014', 'p0025', 'p0046', 'p0063', 'p0077', 'p0187', 'p0212', 'p0223', 'p0235', 'p0239', 'p0263', 'p0269', 'p0001', 'p0008', 'p0150', 'p0157', 'p0167', 'p0202', 'p0213', 'p0231', 'p0241', 'p0254', 'p0276', 'p0306', 'p0031', 'p0038', 'p0048', 'p0068', 'p0163', 'p0175', 'p0192', 'p0242', 'p0257', 'p0272', 'p0284', 'p0288', 'p0006', 'p0023', 'p0051', 'p0099', 'p0106', 'p0118', 'p0125', 'p0186', 'p0189', 'p0217', 'p0238', 'p0290', 'p0024', 'p0026', 'p0056', 'p0132', 'p0139', 'p0182', 'p0203', 'p0207', 'p0216', 'p0221', 'p0226', 'p0251', 'p0017', 'p0040', 'p0153', 'p0166', 'p0195', 'p0232', 'p0253', 'p0268', 'p0280', 'p0294', 'p0308'], 'val_ids': ['p0027', 'p0065', 'p0101', 'p0114', 'p0144', 'p0159', 'p0165', 'p0219', 'p0262', 'p0274', 'p0286', 'p0287'], 'test_ids': ['p0013', 'p0021', 'p0062', 'p0071', 'p0113', 'p0160', 'p0193', 'p0197', 'p0208', 'p0224', 'p0266', 'p0302']}\n",
      "Fold[3] Epoch [1/100] Train Loss: 0.6718 Train Acc: 0.5474, Val AUC: 0.7143\n",
      "Fold[3] Epoch [2/100] Train Loss: 0.6008 Train Acc: 0.6842, Val AUC: 0.8000\n",
      "Fold[3] Epoch [3/100] Train Loss: 0.5441 Train Acc: 0.7895, Val AUC: 0.8286\n",
      "Fold[3] Epoch [4/100] Train Loss: 0.4798 Train Acc: 0.8947, Val AUC: 0.8286\n",
      "Fold[3] Epoch [5/100] Train Loss: 0.4442 Train Acc: 0.8526, Val AUC: 0.8286\n",
      "Fold[3] Epoch [6/100] Train Loss: 0.3682 Train Acc: 0.8737, Val AUC: 0.8571\n",
      "Fold[3] Epoch [7/100] Train Loss: 0.2820 Train Acc: 0.9474, Val AUC: 0.8571\n",
      "Fold[3] Epoch [8/100] Train Loss: 0.2110 Train Acc: 0.9474, Val AUC: 0.9143\n",
      "Fold[3] Epoch [9/100] Train Loss: 0.1454 Train Acc: 0.9684, Val AUC: 0.9143\n",
      "Fold[3] Epoch [10/100] Train Loss: 0.1113 Train Acc: 0.9579, Val AUC: 0.9143\n",
      "Fold[3] Epoch [11/100] Train Loss: 0.0498 Train Acc: 1.0000, Val AUC: 0.9143\n",
      "Fold[3] Epoch [12/100] Train Loss: 0.0503 Train Acc: 0.9895, Val AUC: 0.9143\n",
      "Fold[3] Epoch [13/100] Train Loss: 0.0346 Train Acc: 0.9895, Val AUC: 0.9143\n",
      "Fold[3] Epoch [14/100] Train Loss: 0.0219 Train Acc: 1.0000, Val AUC: 0.9143\n",
      "Fold[3] Epoch [15/100] Train Loss: 0.0295 Train Acc: 0.9895, Val AUC: 0.9143\n",
      "Fold[3] Epoch [16/100] Train Loss: 0.0443 Train Acc: 0.9789, Val AUC: 0.9143\n",
      "Fold[3] Epoch [17/100] Train Loss: 0.0235 Train Acc: 1.0000, Val AUC: 0.9143\n",
      "Fold[3] Epoch [18/100] Train Loss: 0.0175 Train Acc: 0.9895, Val AUC: 0.9143\n",
      "Validation AUC hasn't improved for 5 epochs. Early stopping...\n",
      "Fold[3] completed.\n",
      "\n",
      "Processing fold 4/10...\n",
      "Fold 4 IDs: {'train_ids': ['p0036', 'p0058', 'p0081', 'p0149', 'p0162', 'p0164', 'p0185', 'p0188', 'p0198', 'p0250', 'p0283', 'p0291', 'p0027', 'p0065', 'p0101', 'p0114', 'p0144', 'p0159', 'p0165', 'p0219', 'p0262', 'p0274', 'p0286', 'p0287', 'p0014', 'p0025', 'p0046', 'p0063', 'p0077', 'p0187', 'p0212', 'p0223', 'p0235', 'p0239', 'p0263', 'p0269', 'p0001', 'p0008', 'p0150', 'p0157', 'p0167', 'p0202', 'p0213', 'p0231', 'p0241', 'p0254', 'p0276', 'p0306', 'p0031', 'p0038', 'p0048', 'p0068', 'p0163', 'p0175', 'p0192', 'p0242', 'p0257', 'p0272', 'p0284', 'p0288', 'p0006', 'p0023', 'p0051', 'p0099', 'p0106', 'p0118', 'p0125', 'p0186', 'p0189', 'p0217', 'p0238', 'p0290', 'p0024', 'p0026', 'p0056', 'p0132', 'p0139', 'p0182', 'p0203', 'p0207', 'p0216', 'p0221', 'p0226', 'p0251', 'p0017', 'p0040', 'p0153', 'p0166', 'p0195', 'p0232', 'p0253', 'p0268', 'p0280', 'p0294', 'p0308'], 'val_ids': ['p0013', 'p0021', 'p0062', 'p0071', 'p0113', 'p0160', 'p0193', 'p0197', 'p0208', 'p0224', 'p0266', 'p0302'], 'test_ids': ['p0033', 'p0043', 'p0092', 'p0104', 'p0119', 'p0145', 'p0180', 'p0183', 'p0200', 'p0228', 'p0236', 'p0285']}\n",
      "Fold[4] Epoch [1/100] Train Loss: 0.6782 Train Acc: 0.5474, Val AUC: 0.7812\n",
      "Fold[4] Epoch [2/100] Train Loss: 0.6028 Train Acc: 0.6737, Val AUC: 0.8750\n",
      "Fold[4] Epoch [3/100] Train Loss: 0.5520 Train Acc: 0.7474, Val AUC: 0.8438\n",
      "Fold[4] Epoch [4/100] Train Loss: 0.5135 Train Acc: 0.8316, Val AUC: 0.9062\n",
      "Fold[4] Epoch [5/100] Train Loss: 0.4317 Train Acc: 0.8737, Val AUC: 0.9375\n",
      "Fold[4] Epoch [6/100] Train Loss: 0.3796 Train Acc: 0.8737, Val AUC: 0.9688\n",
      "Fold[4] Epoch [7/100] Train Loss: 0.2980 Train Acc: 0.9158, Val AUC: 0.9688\n",
      "Fold[4] Epoch [8/100] Train Loss: 0.2504 Train Acc: 0.9263, Val AUC: 0.9688\n",
      "Fold[4] Epoch [9/100] Train Loss: 0.2170 Train Acc: 0.9053, Val AUC: 0.9688\n",
      "Fold[4] Epoch [10/100] Train Loss: 0.1184 Train Acc: 1.0000, Val AUC: 0.9688\n",
      "Fold[4] Epoch [11/100] Train Loss: 0.0906 Train Acc: 0.9895, Val AUC: 0.9375\n",
      "Fold[4] Epoch [12/100] Train Loss: 0.0611 Train Acc: 0.9895, Val AUC: 0.9375\n",
      "Fold[4] Epoch [13/100] Train Loss: 0.0395 Train Acc: 1.0000, Val AUC: 0.9375\n",
      "Fold[4] Epoch [14/100] Train Loss: 0.0200 Train Acc: 1.0000, Val AUC: 0.9375\n",
      "Fold[4] Epoch [15/100] Train Loss: 0.0135 Train Acc: 1.0000, Val AUC: 0.9375\n",
      "Fold[4] Epoch [16/100] Train Loss: 0.0118 Train Acc: 1.0000, Val AUC: 0.9375\n",
      "Validation AUC hasn't improved for 5 epochs. Early stopping...\n",
      "Fold[4] completed.\n",
      "\n",
      "Processing fold 5/10...\n",
      "Fold 5 IDs: {'train_ids': ['p0036', 'p0058', 'p0081', 'p0149', 'p0162', 'p0164', 'p0185', 'p0188', 'p0198', 'p0250', 'p0283', 'p0291', 'p0027', 'p0065', 'p0101', 'p0114', 'p0144', 'p0159', 'p0165', 'p0219', 'p0262', 'p0274', 'p0286', 'p0287', 'p0013', 'p0021', 'p0062', 'p0071', 'p0113', 'p0160', 'p0193', 'p0197', 'p0208', 'p0224', 'p0266', 'p0302', 'p0001', 'p0008', 'p0150', 'p0157', 'p0167', 'p0202', 'p0213', 'p0231', 'p0241', 'p0254', 'p0276', 'p0306', 'p0031', 'p0038', 'p0048', 'p0068', 'p0163', 'p0175', 'p0192', 'p0242', 'p0257', 'p0272', 'p0284', 'p0288', 'p0006', 'p0023', 'p0051', 'p0099', 'p0106', 'p0118', 'p0125', 'p0186', 'p0189', 'p0217', 'p0238', 'p0290', 'p0024', 'p0026', 'p0056', 'p0132', 'p0139', 'p0182', 'p0203', 'p0207', 'p0216', 'p0221', 'p0226', 'p0251', 'p0017', 'p0040', 'p0153', 'p0166', 'p0195', 'p0232', 'p0253', 'p0268', 'p0280', 'p0294', 'p0308'], 'val_ids': ['p0033', 'p0043', 'p0092', 'p0104', 'p0119', 'p0145', 'p0180', 'p0183', 'p0200', 'p0228', 'p0236', 'p0285'], 'test_ids': ['p0014', 'p0025', 'p0046', 'p0063', 'p0077', 'p0187', 'p0212', 'p0223', 'p0235', 'p0239', 'p0263', 'p0269']}\n",
      "Fold[5] Epoch [1/100] Train Loss: 0.6503 Train Acc: 0.5684, Val AUC: 0.9375\n",
      "Fold[5] Epoch [2/100] Train Loss: 0.5884 Train Acc: 0.6947, Val AUC: 1.0000\n",
      "Fold[5] Epoch [3/100] Train Loss: 0.5449 Train Acc: 0.6526, Val AUC: 1.0000\n",
      "Fold[5] Epoch [4/100] Train Loss: 0.5020 Train Acc: 0.8316, Val AUC: 1.0000\n",
      "Fold[5] Epoch [5/100] Train Loss: 0.4390 Train Acc: 0.8105, Val AUC: 0.9688\n",
      "Fold[5] Epoch [6/100] Train Loss: 0.3772 Train Acc: 0.8842, Val AUC: 1.0000\n",
      "Fold[5] Epoch [7/100] Train Loss: 0.3156 Train Acc: 0.9158, Val AUC: 1.0000\n",
      "Fold[5] Epoch [8/100] Train Loss: 0.2150 Train Acc: 0.9474, Val AUC: 1.0000\n",
      "Fold[5] Epoch [9/100] Train Loss: 0.2179 Train Acc: 0.9368, Val AUC: 0.9688\n",
      "Fold[5] Epoch [10/100] Train Loss: 0.1390 Train Acc: 0.9579, Val AUC: 0.9688\n",
      "Fold[5] Epoch [11/100] Train Loss: 0.1170 Train Acc: 0.9579, Val AUC: 0.9688\n",
      "Fold[5] Epoch [12/100] Train Loss: 0.0474 Train Acc: 1.0000, Val AUC: 0.9688\n",
      "Validation AUC hasn't improved for 5 epochs. Early stopping...\n",
      "Fold[5] completed.\n",
      "\n",
      "Processing fold 6/10...\n",
      "Fold 6 IDs: {'train_ids': ['p0036', 'p0058', 'p0081', 'p0149', 'p0162', 'p0164', 'p0185', 'p0188', 'p0198', 'p0250', 'p0283', 'p0291', 'p0027', 'p0065', 'p0101', 'p0114', 'p0144', 'p0159', 'p0165', 'p0219', 'p0262', 'p0274', 'p0286', 'p0287', 'p0013', 'p0021', 'p0062', 'p0071', 'p0113', 'p0160', 'p0193', 'p0197', 'p0208', 'p0224', 'p0266', 'p0302', 'p0033', 'p0043', 'p0092', 'p0104', 'p0119', 'p0145', 'p0180', 'p0183', 'p0200', 'p0228', 'p0236', 'p0285', 'p0031', 'p0038', 'p0048', 'p0068', 'p0163', 'p0175', 'p0192', 'p0242', 'p0257', 'p0272', 'p0284', 'p0288', 'p0006', 'p0023', 'p0051', 'p0099', 'p0106', 'p0118', 'p0125', 'p0186', 'p0189', 'p0217', 'p0238', 'p0290', 'p0024', 'p0026', 'p0056', 'p0132', 'p0139', 'p0182', 'p0203', 'p0207', 'p0216', 'p0221', 'p0226', 'p0251', 'p0017', 'p0040', 'p0153', 'p0166', 'p0195', 'p0232', 'p0253', 'p0268', 'p0280', 'p0294', 'p0308'], 'val_ids': ['p0014', 'p0025', 'p0046', 'p0063', 'p0077', 'p0187', 'p0212', 'p0223', 'p0235', 'p0239', 'p0263', 'p0269'], 'test_ids': ['p0001', 'p0008', 'p0150', 'p0157', 'p0167', 'p0202', 'p0213', 'p0231', 'p0241', 'p0254', 'p0276', 'p0306']}\n",
      "Fold[6] Epoch [1/100] Train Loss: 0.6857 Train Acc: 0.4737, Val AUC: 0.7188\n",
      "Fold[6] Epoch [2/100] Train Loss: 0.5857 Train Acc: 0.7158, Val AUC: 0.6875\n",
      "Fold[6] Epoch [3/100] Train Loss: 0.5405 Train Acc: 0.7895, Val AUC: 0.7500\n",
      "Fold[6] Epoch [4/100] Train Loss: 0.4854 Train Acc: 0.8105, Val AUC: 0.8125\n",
      "Fold[6] Epoch [5/100] Train Loss: 0.4267 Train Acc: 0.8842, Val AUC: 0.8125\n",
      "Fold[6] Epoch [6/100] Train Loss: 0.3473 Train Acc: 0.9158, Val AUC: 0.8125\n",
      "Fold[6] Epoch [7/100] Train Loss: 0.2569 Train Acc: 0.9474, Val AUC: 0.7812\n",
      "Fold[6] Epoch [8/100] Train Loss: 0.1921 Train Acc: 0.9684, Val AUC: 0.7188\n",
      "Fold[6] Epoch [9/100] Train Loss: 0.1346 Train Acc: 0.9789, Val AUC: 0.7188\n",
      "Fold[6] Epoch [10/100] Train Loss: 0.1116 Train Acc: 0.9789, Val AUC: 0.6875\n",
      "Fold[6] Epoch [11/100] Train Loss: 0.0750 Train Acc: 0.9895, Val AUC: 0.6875\n",
      "Fold[6] Epoch [12/100] Train Loss: 0.0442 Train Acc: 0.9895, Val AUC: 0.6875\n",
      "Fold[6] Epoch [13/100] Train Loss: 0.0266 Train Acc: 1.0000, Val AUC: 0.6875\n",
      "Fold[6] Epoch [14/100] Train Loss: 0.0190 Train Acc: 1.0000, Val AUC: 0.6562\n",
      "Validation AUC hasn't improved for 5 epochs. Early stopping...\n",
      "Fold[6] completed.\n",
      "\n",
      "Processing fold 7/10...\n",
      "Fold 7 IDs: {'train_ids': ['p0036', 'p0058', 'p0081', 'p0149', 'p0162', 'p0164', 'p0185', 'p0188', 'p0198', 'p0250', 'p0283', 'p0291', 'p0027', 'p0065', 'p0101', 'p0114', 'p0144', 'p0159', 'p0165', 'p0219', 'p0262', 'p0274', 'p0286', 'p0287', 'p0013', 'p0021', 'p0062', 'p0071', 'p0113', 'p0160', 'p0193', 'p0197', 'p0208', 'p0224', 'p0266', 'p0302', 'p0033', 'p0043', 'p0092', 'p0104', 'p0119', 'p0145', 'p0180', 'p0183', 'p0200', 'p0228', 'p0236', 'p0285', 'p0014', 'p0025', 'p0046', 'p0063', 'p0077', 'p0187', 'p0212', 'p0223', 'p0235', 'p0239', 'p0263', 'p0269', 'p0006', 'p0023', 'p0051', 'p0099', 'p0106', 'p0118', 'p0125', 'p0186', 'p0189', 'p0217', 'p0238', 'p0290', 'p0024', 'p0026', 'p0056', 'p0132', 'p0139', 'p0182', 'p0203', 'p0207', 'p0216', 'p0221', 'p0226', 'p0251', 'p0017', 'p0040', 'p0153', 'p0166', 'p0195', 'p0232', 'p0253', 'p0268', 'p0280', 'p0294', 'p0308'], 'val_ids': ['p0001', 'p0008', 'p0150', 'p0157', 'p0167', 'p0202', 'p0213', 'p0231', 'p0241', 'p0254', 'p0276', 'p0306'], 'test_ids': ['p0031', 'p0038', 'p0048', 'p0068', 'p0163', 'p0175', 'p0192', 'p0242', 'p0257', 'p0272', 'p0284', 'p0288']}\n",
      "Fold[7] Epoch [1/100] Train Loss: 0.6680 Train Acc: 0.4737, Val AUC: 0.4375\n",
      "Fold[7] Epoch [2/100] Train Loss: 0.5850 Train Acc: 0.7579, Val AUC: 0.5625\n",
      "Fold[7] Epoch [3/100] Train Loss: 0.5581 Train Acc: 0.6947, Val AUC: 0.5625\n",
      "Fold[7] Epoch [4/100] Train Loss: 0.4845 Train Acc: 0.8421, Val AUC: 0.7500\n",
      "Fold[7] Epoch [5/100] Train Loss: 0.4355 Train Acc: 0.9053, Val AUC: 0.6562\n",
      "Fold[7] Epoch [6/100] Train Loss: 0.3487 Train Acc: 0.8842, Val AUC: 0.6562\n",
      "Fold[7] Epoch [7/100] Train Loss: 0.2920 Train Acc: 0.9263, Val AUC: 0.6875\n",
      "Fold[7] Epoch [8/100] Train Loss: 0.2139 Train Acc: 0.9684, Val AUC: 0.6250\n",
      "Fold[7] Epoch [9/100] Train Loss: 0.1556 Train Acc: 0.9474, Val AUC: 0.6875\n",
      "Fold[7] Epoch [10/100] Train Loss: 0.1141 Train Acc: 0.9789, Val AUC: 0.6562\n",
      "Fold[7] Epoch [11/100] Train Loss: 0.0549 Train Acc: 1.0000, Val AUC: 0.6875\n",
      "Fold[7] Epoch [12/100] Train Loss: 0.0400 Train Acc: 1.0000, Val AUC: 0.6562\n",
      "Fold[7] Epoch [13/100] Train Loss: 0.0197 Train Acc: 1.0000, Val AUC: 0.6562\n",
      "Fold[7] Epoch [14/100] Train Loss: 0.0143 Train Acc: 1.0000, Val AUC: 0.6562\n",
      "Validation AUC hasn't improved for 5 epochs. Early stopping...\n",
      "Fold[7] completed.\n",
      "\n",
      "Processing fold 8/10...\n",
      "Fold 8 IDs: {'train_ids': ['p0036', 'p0058', 'p0081', 'p0149', 'p0162', 'p0164', 'p0185', 'p0188', 'p0198', 'p0250', 'p0283', 'p0291', 'p0027', 'p0065', 'p0101', 'p0114', 'p0144', 'p0159', 'p0165', 'p0219', 'p0262', 'p0274', 'p0286', 'p0287', 'p0013', 'p0021', 'p0062', 'p0071', 'p0113', 'p0160', 'p0193', 'p0197', 'p0208', 'p0224', 'p0266', 'p0302', 'p0033', 'p0043', 'p0092', 'p0104', 'p0119', 'p0145', 'p0180', 'p0183', 'p0200', 'p0228', 'p0236', 'p0285', 'p0014', 'p0025', 'p0046', 'p0063', 'p0077', 'p0187', 'p0212', 'p0223', 'p0235', 'p0239', 'p0263', 'p0269', 'p0001', 'p0008', 'p0150', 'p0157', 'p0167', 'p0202', 'p0213', 'p0231', 'p0241', 'p0254', 'p0276', 'p0306', 'p0024', 'p0026', 'p0056', 'p0132', 'p0139', 'p0182', 'p0203', 'p0207', 'p0216', 'p0221', 'p0226', 'p0251', 'p0017', 'p0040', 'p0153', 'p0166', 'p0195', 'p0232', 'p0253', 'p0268', 'p0280', 'p0294', 'p0308'], 'val_ids': ['p0031', 'p0038', 'p0048', 'p0068', 'p0163', 'p0175', 'p0192', 'p0242', 'p0257', 'p0272', 'p0284', 'p0288'], 'test_ids': ['p0006', 'p0023', 'p0051', 'p0099', 'p0106', 'p0118', 'p0125', 'p0186', 'p0189', 'p0217', 'p0238', 'p0290']}\n",
      "Fold[8] Epoch [1/100] Train Loss: 0.6794 Train Acc: 0.5053, Val AUC: 0.8750\n",
      "Fold[8] Epoch [2/100] Train Loss: 0.6069 Train Acc: 0.7368, Val AUC: 0.8750\n",
      "Fold[8] Epoch [3/100] Train Loss: 0.5637 Train Acc: 0.8632, Val AUC: 0.8438\n",
      "Fold[8] Epoch [4/100] Train Loss: 0.5090 Train Acc: 0.7789, Val AUC: 0.8750\n",
      "Fold[8] Epoch [5/100] Train Loss: 0.4435 Train Acc: 0.8842, Val AUC: 0.9375\n",
      "Fold[8] Epoch [6/100] Train Loss: 0.3980 Train Acc: 0.9368, Val AUC: 0.9375\n",
      "Fold[8] Epoch [7/100] Train Loss: 0.3131 Train Acc: 0.9158, Val AUC: 0.8750\n",
      "Fold[8] Epoch [8/100] Train Loss: 0.2702 Train Acc: 0.9263, Val AUC: 0.9375\n",
      "Fold[8] Epoch [9/100] Train Loss: 0.1977 Train Acc: 0.9579, Val AUC: 0.8750\n",
      "Fold[8] Epoch [10/100] Train Loss: 0.1288 Train Acc: 0.9789, Val AUC: 0.9062\n",
      "Fold[8] Epoch [11/100] Train Loss: 0.1032 Train Acc: 0.9895, Val AUC: 0.7812\n",
      "Fold[8] Epoch [12/100] Train Loss: 0.0747 Train Acc: 0.9789, Val AUC: 0.8438\n",
      "Fold[8] Epoch [13/100] Train Loss: 0.0558 Train Acc: 1.0000, Val AUC: 0.8125\n",
      "Fold[8] Epoch [14/100] Train Loss: 0.0517 Train Acc: 0.9895, Val AUC: 0.8750\n",
      "Fold[8] Epoch [15/100] Train Loss: 0.0410 Train Acc: 0.9895, Val AUC: 0.8438\n",
      "Validation AUC hasn't improved for 5 epochs. Early stopping...\n",
      "Fold[8] completed.\n",
      "\n",
      "Processing fold 9/10...\n",
      "Fold 9 IDs: {'train_ids': ['p0036', 'p0058', 'p0081', 'p0149', 'p0162', 'p0164', 'p0185', 'p0188', 'p0198', 'p0250', 'p0283', 'p0291', 'p0027', 'p0065', 'p0101', 'p0114', 'p0144', 'p0159', 'p0165', 'p0219', 'p0262', 'p0274', 'p0286', 'p0287', 'p0013', 'p0021', 'p0062', 'p0071', 'p0113', 'p0160', 'p0193', 'p0197', 'p0208', 'p0224', 'p0266', 'p0302', 'p0033', 'p0043', 'p0092', 'p0104', 'p0119', 'p0145', 'p0180', 'p0183', 'p0200', 'p0228', 'p0236', 'p0285', 'p0014', 'p0025', 'p0046', 'p0063', 'p0077', 'p0187', 'p0212', 'p0223', 'p0235', 'p0239', 'p0263', 'p0269', 'p0001', 'p0008', 'p0150', 'p0157', 'p0167', 'p0202', 'p0213', 'p0231', 'p0241', 'p0254', 'p0276', 'p0306', 'p0031', 'p0038', 'p0048', 'p0068', 'p0163', 'p0175', 'p0192', 'p0242', 'p0257', 'p0272', 'p0284', 'p0288', 'p0017', 'p0040', 'p0153', 'p0166', 'p0195', 'p0232', 'p0253', 'p0268', 'p0280', 'p0294', 'p0308'], 'val_ids': ['p0006', 'p0023', 'p0051', 'p0099', 'p0106', 'p0118', 'p0125', 'p0186', 'p0189', 'p0217', 'p0238', 'p0290'], 'test_ids': ['p0024', 'p0026', 'p0056', 'p0132', 'p0139', 'p0182', 'p0203', 'p0207', 'p0216', 'p0221', 'p0226', 'p0251']}\n",
      "Fold[9] Epoch [1/100] Train Loss: 0.6442 Train Acc: 0.5789, Val AUC: 0.6250\n",
      "Fold[9] Epoch [2/100] Train Loss: 0.5786 Train Acc: 0.7474, Val AUC: 0.7500\n",
      "Fold[9] Epoch [3/100] Train Loss: 0.5371 Train Acc: 0.6526, Val AUC: 0.7500\n",
      "Fold[9] Epoch [4/100] Train Loss: 0.4747 Train Acc: 0.8737, Val AUC: 0.7812\n",
      "Fold[9] Epoch [5/100] Train Loss: 0.3817 Train Acc: 0.8526, Val AUC: 0.7812\n",
      "Fold[9] Epoch [6/100] Train Loss: 0.3325 Train Acc: 0.9053, Val AUC: 0.7812\n",
      "Fold[9] Epoch [7/100] Train Loss: 0.2405 Train Acc: 0.9368, Val AUC: 0.7812\n",
      "Fold[9] Epoch [8/100] Train Loss: 0.1981 Train Acc: 0.9368, Val AUC: 0.7812\n",
      "Fold[9] Epoch [9/100] Train Loss: 0.1259 Train Acc: 0.9789, Val AUC: 0.8125\n",
      "Fold[9] Epoch [10/100] Train Loss: 0.0751 Train Acc: 1.0000, Val AUC: 0.8438\n",
      "Fold[9] Epoch [11/100] Train Loss: 0.0805 Train Acc: 0.9789, Val AUC: 0.9062\n",
      "Fold[9] Epoch [12/100] Train Loss: 0.0399 Train Acc: 1.0000, Val AUC: 0.9062\n",
      "Fold[9] Epoch [13/100] Train Loss: 0.0373 Train Acc: 0.9684, Val AUC: 0.9062\n",
      "Fold[9] Epoch [14/100] Train Loss: 0.0313 Train Acc: 1.0000, Val AUC: 0.9375\n",
      "Fold[9] Epoch [15/100] Train Loss: 0.0182 Train Acc: 0.9895, Val AUC: 0.9688\n",
      "Fold[9] Epoch [16/100] Train Loss: 0.0089 Train Acc: 1.0000, Val AUC: 0.9688\n",
      "Fold[9] Epoch [17/100] Train Loss: 0.0082 Train Acc: 1.0000, Val AUC: 0.9688\n",
      "Fold[9] Epoch [18/100] Train Loss: 0.0095 Train Acc: 1.0000, Val AUC: 0.9688\n",
      "Fold[9] Epoch [19/100] Train Loss: 0.0056 Train Acc: 1.0000, Val AUC: 0.9688\n",
      "Fold[9] Epoch [20/100] Train Loss: 0.0049 Train Acc: 1.0000, Val AUC: 0.9688\n",
      "Fold[9] Epoch [21/100] Train Loss: 0.0059 Train Acc: 1.0000, Val AUC: 0.9375\n",
      "Fold[9] Epoch [22/100] Train Loss: 0.0042 Train Acc: 1.0000, Val AUC: 0.9375\n",
      "Fold[9] Epoch [23/100] Train Loss: 0.0031 Train Acc: 1.0000, Val AUC: 0.9375\n",
      "Fold[9] Epoch [24/100] Train Loss: 0.0032 Train Acc: 1.0000, Val AUC: 0.9375\n",
      "Fold[9] Epoch [25/100] Train Loss: 0.0065 Train Acc: 1.0000, Val AUC: 0.9375\n",
      "Validation AUC hasn't improved for 5 epochs. Early stopping...\n",
      "Fold[9] completed.\n",
      "\n",
      "Processing fold 10/10...\n",
      "Fold 10 IDs: {'train_ids': ['p0036', 'p0058', 'p0081', 'p0149', 'p0162', 'p0164', 'p0185', 'p0188', 'p0198', 'p0250', 'p0283', 'p0291', 'p0027', 'p0065', 'p0101', 'p0114', 'p0144', 'p0159', 'p0165', 'p0219', 'p0262', 'p0274', 'p0286', 'p0287', 'p0013', 'p0021', 'p0062', 'p0071', 'p0113', 'p0160', 'p0193', 'p0197', 'p0208', 'p0224', 'p0266', 'p0302', 'p0033', 'p0043', 'p0092', 'p0104', 'p0119', 'p0145', 'p0180', 'p0183', 'p0200', 'p0228', 'p0236', 'p0285', 'p0014', 'p0025', 'p0046', 'p0063', 'p0077', 'p0187', 'p0212', 'p0223', 'p0235', 'p0239', 'p0263', 'p0269', 'p0001', 'p0008', 'p0150', 'p0157', 'p0167', 'p0202', 'p0213', 'p0231', 'p0241', 'p0254', 'p0276', 'p0306', 'p0031', 'p0038', 'p0048', 'p0068', 'p0163', 'p0175', 'p0192', 'p0242', 'p0257', 'p0272', 'p0284', 'p0288', 'p0006', 'p0023', 'p0051', 'p0099', 'p0106', 'p0118', 'p0125', 'p0186', 'p0189', 'p0217', 'p0238', 'p0290'], 'val_ids': ['p0024', 'p0026', 'p0056', 'p0132', 'p0139', 'p0182', 'p0203', 'p0207', 'p0216', 'p0221', 'p0226', 'p0251'], 'test_ids': ['p0017', 'p0040', 'p0153', 'p0166', 'p0195', 'p0232', 'p0253', 'p0268', 'p0280', 'p0294', 'p0308']}\n",
      "Fold[10] Epoch [1/100] Train Loss: 0.6558 Train Acc: 0.5417, Val AUC: 0.5625\n",
      "Fold[10] Epoch [2/100] Train Loss: 0.5817 Train Acc: 0.7292, Val AUC: 0.5312\n",
      "Fold[10] Epoch [3/100] Train Loss: 0.5242 Train Acc: 0.7917, Val AUC: 0.5625\n",
      "Fold[10] Epoch [4/100] Train Loss: 0.4655 Train Acc: 0.8229, Val AUC: 0.5938\n",
      "Fold[10] Epoch [5/100] Train Loss: 0.3628 Train Acc: 0.8750, Val AUC: 0.5938\n",
      "Fold[10] Epoch [6/100] Train Loss: 0.3032 Train Acc: 0.8958, Val AUC: 0.5938\n",
      "Fold[10] Epoch [7/100] Train Loss: 0.2189 Train Acc: 0.9271, Val AUC: 0.5938\n",
      "Fold[10] Epoch [8/100] Train Loss: 0.1635 Train Acc: 0.9375, Val AUC: 0.6250\n",
      "Fold[10] Epoch [9/100] Train Loss: 0.1038 Train Acc: 1.0000, Val AUC: 0.6250\n",
      "Fold[10] Epoch [10/100] Train Loss: 0.0542 Train Acc: 1.0000, Val AUC: 0.6250\n",
      "Fold[10] Epoch [11/100] Train Loss: 0.0339 Train Acc: 1.0000, Val AUC: 0.6562\n",
      "Fold[10] Epoch [12/100] Train Loss: 0.0265 Train Acc: 1.0000, Val AUC: 0.6562\n",
      "Fold[10] Epoch [13/100] Train Loss: 0.0627 Train Acc: 0.9792, Val AUC: 0.6562\n",
      "Fold[10] Epoch [14/100] Train Loss: 0.0096 Train Acc: 1.0000, Val AUC: 0.6562\n",
      "Fold[10] Epoch [15/100] Train Loss: 0.0230 Train Acc: 1.0000, Val AUC: 0.6250\n",
      "Fold[10] Epoch [16/100] Train Loss: 0.0110 Train Acc: 1.0000, Val AUC: 0.6250\n",
      "Fold[10] Epoch [17/100] Train Loss: 0.0096 Train Acc: 1.0000, Val AUC: 0.6875\n",
      "Fold[10] Epoch [18/100] Train Loss: 0.0090 Train Acc: 1.0000, Val AUC: 0.6875\n",
      "Fold[10] Epoch [19/100] Train Loss: 0.0054 Train Acc: 1.0000, Val AUC: 0.6250\n",
      "Fold[10] Epoch [20/100] Train Loss: 0.0068 Train Acc: 1.0000, Val AUC: 0.6250\n",
      "Fold[10] Epoch [21/100] Train Loss: 0.0059 Train Acc: 1.0000, Val AUC: 0.6250\n",
      "Fold[10] Epoch [22/100] Train Loss: 0.0033 Train Acc: 1.0000, Val AUC: 0.6250\n",
      "Fold[10] Epoch [23/100] Train Loss: 0.0027 Train Acc: 1.0000, Val AUC: 0.6250\n",
      "Fold[10] Epoch [24/100] Train Loss: 0.0031 Train Acc: 1.0000, Val AUC: 0.6250\n",
      "Fold[10] Epoch [25/100] Train Loss: 0.0041 Train Acc: 1.0000, Val AUC: 0.6250\n",
      "Fold[10] Epoch [26/100] Train Loss: 0.0018 Train Acc: 1.0000, Val AUC: 0.6250\n",
      "Fold[10] Epoch [27/100] Train Loss: 0.0023 Train Acc: 1.0000, Val AUC: 0.6250\n",
      "Validation AUC hasn't improved for 5 epochs. Early stopping...\n",
      "Fold[10] completed.\n",
      "\n",
      "all test label: [1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0.]\n",
      "all test probability: [9.50442255e-01 6.11352595e-03 1.57465646e-03 1.60617707e-03\n",
      " 5.91410518e-01 3.69308196e-04 5.82737863e-01 9.98877823e-01\n",
      " 9.81709421e-01 7.49916828e-04 4.67512495e-04 4.83195501e-04\n",
      " 1.33190870e-01 9.99317527e-01 9.51919973e-01 4.19996381e-01\n",
      " 1.42978143e-03 1.91340335e-02 1.53675973e-01 8.53852020e-04\n",
      " 9.98843789e-01 9.66976804e-04 1.43753467e-02 2.31692716e-01\n",
      " 1.26376934e-03 9.89107847e-01 9.66097474e-01 4.26666290e-01\n",
      " 9.36925295e-04 7.27716625e-01 2.22450192e-03 2.81362623e-01\n",
      " 1.35727657e-03 6.12604856e-01 1.05582247e-03 1.66080929e-02\n",
      " 9.92668094e-04 2.49520107e-03 1.80918321e-01 9.74786162e-01\n",
      " 9.94242728e-01 1.16916811e-02 4.70340298e-03 2.76860013e-03\n",
      " 1.40908652e-03 3.10348161e-03 2.01537670e-03 2.12276005e-03\n",
      " 5.46427131e-01 3.12871858e-02 7.64360189e-01 4.17952836e-02\n",
      " 5.10057807e-01 4.74552304e-01 2.98534006e-01 1.22177623e-01\n",
      " 3.15886810e-02 9.57259536e-01 2.92007625e-01 1.25722945e-01\n",
      " 9.70988870e-01 5.20699611e-03 7.04038367e-02 1.36856884e-01\n",
      " 6.05192855e-02 1.54568488e-03 9.40184295e-01 3.28327529e-02\n",
      " 6.94182003e-03 5.71344607e-03 1.42264701e-02 2.49288619e-01\n",
      " 7.06806183e-01 1.39657792e-03 1.26212344e-01 8.16592336e-01\n",
      " 9.12678614e-02 2.82019516e-03 2.22780230e-03 3.43347178e-03\n",
      " 4.35351022e-03 2.13024463e-03 2.45586014e-03 6.64702177e-01\n",
      " 1.49766274e-03 1.79588757e-02 4.95087504e-01 1.04873935e-02\n",
      " 2.99239047e-02 9.93369401e-01 3.50694754e-03 4.79760677e-01\n",
      " 9.77648675e-01 4.39456198e-03 1.56978215e-03 6.67227685e-01\n",
      " 9.97338116e-01 9.70727086e-01 7.02629507e-01 1.89529930e-03\n",
      " 9.99277413e-01 4.62308235e-04 3.29386559e-04 2.21620753e-01\n",
      " 6.15124067e-04 3.91731232e-01 3.58679856e-04 9.97764945e-01\n",
      " 9.98252690e-01 9.99522924e-01 5.47968200e-04 3.20038199e-03\n",
      " 7.86079653e-03 5.31852013e-04 5.85875630e-01 4.02426551e-04\n",
      " 6.83000311e-04 4.35907627e-04 1.72076508e-01]\n",
      "\n",
      "overall AUC: 0.8120\n",
      "overall Sensitivity (Recall): 0.6667\n",
      "overall Specificity: 0.8442\n",
      "overall F1 Score: 0.6829\n",
      "overall Accuracy: 0.7815\n",
      "overall MAE: 0.2185\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# hyper parameters \n",
    "num_folds = 10\n",
    "batch_size = 16\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 100\n",
    "patience = 10#  early stopping\n",
    "\n",
    "# train  (10-fold)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "all_y_test = []\n",
    "all_y_pred_proba = []\n",
    "\n",
    "for fold in range(num_folds):\n",
    "    print(f\"Processing fold {fold + 1}/{num_folds}...\")\n",
    "    \n",
    "    test_fold = fold\n",
    "    val_fold = (fold - 1) % num_folds \n",
    "    train_folds = [i for i in range(num_folds) if i != test_fold and i != val_fold]\n",
    "    \n",
    "    train_patient_ids = [pid for pid, f in patient_fold_mapping.items() if f in train_folds]\n",
    "    val_patient_ids = [pid for pid, f in patient_fold_mapping.items() if f == val_fold]\n",
    "    test_patient_ids = [pid for pid, f in patient_fold_mapping.items() if f == test_fold]\n",
    "    \n",
    "    fold_ids = {\n",
    "        \"train_ids\": train_patient_ids,\n",
    "        \"val_ids\": val_patient_ids,\n",
    "        \"test_ids\": test_patient_ids\n",
    "    }\n",
    "    print(f\"Fold {fold + 1} IDs: {fold_ids}\")\n",
    "    \n",
    "    # split data\n",
    "    df_train = df[df['patient_id'].isin(train_patient_ids)].copy()\n",
    "    df_valid = df[df['patient_id'].isin(val_patient_ids)].copy()\n",
    "    df_test = df[df['patient_id'].isin(test_patient_ids)].copy()\n",
    "    \n",
    "    # norm only for clinical data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df_train[clinical_feature_cols])   # fit on train dataset \n",
    "    \n",
    "    df_train[clinical_feature_cols] = scaler.transform(df_train[clinical_feature_cols])\n",
    "    df_valid[clinical_feature_cols] = scaler.transform(df_valid[clinical_feature_cols])\n",
    "    df_test[clinical_feature_cols] = scaler.transform(df_test[clinical_feature_cols])\n",
    "    \n",
    "    # create dataset\n",
    "    train_dataset = ClinicalImageDataset(df_train, clinical_feature_cols, label_col)\n",
    "    val_dataset = ClinicalImageDataset(df_valid, clinical_feature_cols, label_col)\n",
    "    test_dataset = ClinicalImageDataset(df_test, clinical_feature_cols, label_col)\n",
    "    \n",
    "    # dataloader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # model\n",
    "    clinical_dim = len(clinical_feature_cols)  # 85\n",
    "    embed_dim = 512     # embedding dim\n",
    "    num_heads = 4\n",
    "    dropout = 0.2\n",
    "    \n",
    "    model = ClinicalImageFusionModel(clinical_dim=clinical_dim,\n",
    "                                     embed_dim=embed_dim,\n",
    "                                     num_heads=num_heads,\n",
    "                                     dropout=dropout).to(device)\n",
    "    \n",
    "    # loss and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # train\n",
    "    no_improve_count = 0\n",
    "    best_val_auc = 0.0\n",
    "    best_model_weights = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_auc = validate_auc(model, val_loader, device)\n",
    "        print(f\"Fold[{fold+1}] Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f} Train Acc: {train_acc:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "    \n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            best_model_weights = model.state_dict()\n",
    "            no_improve_count = 0\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "    \n",
    "        if no_improve_count >= patience:\n",
    "            print(\"Validation AUC hasn't improved for 5 epochs. Early stopping...\")\n",
    "            break\n",
    "    \n",
    "    # load best model\n",
    "    if best_model_weights is not None:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "    \n",
    "    # inference in test data\n",
    "    y_test_fold, y_pred_proba_fold = test_inference(model, test_loader, device)\n",
    "    all_y_test.append(y_test_fold)\n",
    "    all_y_pred_proba.append(y_pred_proba_fold)\n",
    "    \n",
    "    print(f\"Fold[{fold+1}] completed.\\n\")\n",
    "\n",
    "# collect all test \n",
    "all_y_test = np.concatenate(all_y_test)\n",
    "all_y_pred_proba = np.concatenate(all_y_pred_proba)\n",
    "\n",
    "\n",
    "best_threshold = 0.36\n",
    "all_y_pred = np.where(all_y_pred_proba > best_threshold, 1, 0)\n",
    "\n",
    " ###confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(all_y_test, all_y_pred).ravel()\n",
    "acc = accuracy_score(all_y_test, all_y_pred)\n",
    "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0 \n",
    "f1 = f1_score(all_y_test, all_y_pred) \n",
    "mae = mean_absolute_error(all_y_test, all_y_pred)\n",
    "auc = roc_auc_score(all_y_test, all_y_pred_proba)\n",
    "\n",
    "\n",
    "print(\"all test label:\", all_y_test)\n",
    "print(\"all test probability:\", all_y_pred_proba)\n",
    "print(f\"\\noverall AUC: {auc:.4f}\")\n",
    "print(f\"overall Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"overall Specificity: {specificity:.4f}\")\n",
    "print(f\"overall F1 Score: {f1:.4f}\")\n",
    "print(f\"overall Accuracy: {acc:.4f}\")\n",
    "print(f\"overall MAE: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
